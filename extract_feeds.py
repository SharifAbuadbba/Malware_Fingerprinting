import boto3
import os 
import csv  
import re

def create_files(header,fname):

    with open(fname, 'w', encoding='UTF8') as f:
        writer = csv.writer(f)

    # write the header
        writer.writerow(header)


def extract_features(filename,minfeed_list,total,win_count):
    false_count = 0
    """ Receive a feed as json structure and extract important fields"""
    
 

    for i in range(len(minfeed_list)):
        dic_feed=eval(str(minfeed_list[i]))
        print(f'total: {total}')
        total+=1
        try:
            filetype= dic_feed['attributes']['exiftool']['FileType']
        except KeyError:
            false_count+=1
            #non_fileType.append(new_data)
            pass

def extract_features(filename,minfeed_list,total,win_count):
    false_count = 0
    """ Receive a feed as json structure and extract important fields"""
    
    with open('feedsV2.csv', 'a', encoding='UTF8') as feeds:
        feeds_writer = csv.writer(feeds)

        for i in range(len(minfeed_list)):
            dic_feed=eval(str(minfeed_list[i]))
            print(f'total: {total}')
            total+=1
            try:
                filetype= dic_feed['attributes']['exiftool']['FileType']
                
                
                if 'Win' in filetype:
                    #Win_files.append(new_data)
                    print(f'filetype: {filetype}')
                    print(f'win_count: {win_count}')
                    win_count+=1
                    #----------------------
                    try:  #Feature 1 
                        authentihash = dic_feed['attributes']['authentihash']
                    except KeyError:
                        authentihash = ""
                        
                #     if dic_feed['attributes']['authentihash'] in dic_feed:
                #         authentihash = dic_feed['attributes']['authentihash']
                #     else: 
                #         authentihash = 0
                    #print(f'authentihash: {authentihash}')

                    # try:  #Feature 2
                    #     filetype= dic_feed['attributes']['exiftool']['FileType']
                    # except KeyError:
                    #     filetype = ""

                        
                    #print(f'filetype: {filetype}')

                    try:  #Feature 3
                        codesize = int(dic_feed['attributes']['exiftool']['CodeSize'])
                    except KeyError:
                        codesize = -1
                        

                    try:  #Feature 4
                        datetime = (dic_feed['attributes']['exiftool']['TimeStamp']).split(":")
                        if isinstance(datetime[0], int):
                            timestamp = int(datetime[0])
                        else:
                            buffer=re.search(r"(\d{4})", str(datetime)).group(1)
                            timestamp=int(buffer)
                    except KeyError:
                        timestamp = -1 

                    #print(f'timestamp: {timestamp}')

                    try:  #Feature 5
                        malicious= int(dic_feed['attributes']['last_analysis_stats']['malicious'])
                    except KeyError:
                        malicious = -1 
                        
                
                
                    try:  #Feature 6
                        undetected = int(dic_feed['attributes']['last_analysis_stats']['undetected']) 
                    except KeyError:
                        undetected = -1
                        

                        
                    #print(f'undetected: {undetected}')

                    try:  #Feature 7
                        file_md5 = dic_feed['attributes']['md5']
                    except KeyError:
                        file_md5 = ""
                        
                    try: #Feature 9
                        sha1 = dic_feed['attributes']['sha1']
                    except KeyError:
                        sha1 = ""

                    try: #Feature 10
                        sha256 = dic_feed['attributes']['sha256']
                    except KeyError:
                        sha256 = ""  

                    #print(f'file_md5: {file_md5}')

                    try: #Feature 8
                        imp_hash = dic_feed['attributes']['pe_info']['imphash']
                    except KeyError:
                        imp_hash = ""
                        #imp_hash = str(total)
                                #----------------------
                    try:
                        icon = dic_feed['attributes']['pe_info']['main_icon']
                        print(f'---------------------------------------------------------------- icon {icon}')
                        icon_dhash = dic_feed['attributes']['pe_info']['main_icon']['dhash']
                    except KeyError:
                        icon_dhash = ""

                    try:
                        icon_raw_md5 = dic_feed['attributes']['pe_info']['main_icon']['raw_md5']
                    except KeyError:
                        icon_raw_md5 = ""

                    try:
                        header_hash = dic_feed['attributes']['pe_info']["rich_pe_header_hash"]
                    
                    except KeyError:
                        header_hash = ""

                    #print(f'sha1: {sha1}')

                    #print(f'sha256: {sha256}')

                    try: #Feature 11
                        ssdeep = dic_feed['attributes']['ssdeep']
                        ssdeep_parts = ssdeep.split(":")
                        ssdeep_blocksize = ssdeep_parts[0]
                        ssdeep_hash1 = ssdeep_parts[1]
                        ssdeep_hash2 = ssdeep_parts[2]
                    except KeyError:
                        ssdeep = ""   
                        
                    try: #Feature 12
                        tlsh = dic_feed['attributes']['tlsh']
                    except KeyError:
                        tlsh = ""   


                    #print(f'tlsh: {tlsh}')
                    try: #Feature 13
                        vhash = dic_feed['attributes']['vhash']
                    except KeyError:
                        vhash = ""  

                    
            
            
                    # ---- Collect resources features ----
                    try:
                        resource_details = dic_feed['attributes']['pe_info']['resource_details']
                        resources_len = len(resource_details)
                        #print(f'Resources {len(resource_details)}')
                        with open('resourcesV4.csv', 'a', encoding='UTF8') as f_res:
                            res_writer = csv.writer(f_res)
                                                   
                            for item in resource_details:
                                
                                res_chi2 = item['chi2']
                                res_entropy = item['entropy']
                                res_filetype = item['filetype']
                                res_sha256 = item['sha256']
                                typeX = item['type']
                                
                                # Write into resources csv file!
                                res_writer.writerow([filename,win_count,sha256,imp_hash,res_chi2,res_entropy,res_filetype,res_sha256,typeX])

                    except KeyError:
                        resource_details = ""
                        resources_len = 0
                    # ---- Collect sections features -----
                    try:
                        sections = dic_feed['attributes']['pe_info']['sections']
                        sections_len = len(sections)

                        with open('sectionsV4.csv', 'a', encoding='UTF8') as f_sec:
                            sec_writer = csv.writer(f_sec)

                            
                        
                            for item in sections:
                                
                                sec_chi2 = item['chi2']
                                sec_entropy = item['entropy']
                                sec_md5 = item['md5']
                                raw_size = item['raw_size']
                                virtual_size=item['virtual_size']
                                virtual_address=item['virtual_address']
                                sec_name=item['name']
                                
                                # Write into sections csv file!
                                sec_writer.writerow([filename,win_count,sha256,imp_hash,sec_chi2,sec_entropy,sec_md5,raw_size,virtual_size,virtual_address,sec_name])
                        
                    except KeyError:
                        sections = ""
                        sections_len = 0

                    # ---------- Write the feed metadata ----------
                    feeds_writer.writerow([filename,win_count,authentihash,filetype, codesize, timestamp, malicious, undetected, resources_len, sections_len, file_md5,  sha1, sha256, imp_hash, icon_dhash, icon_raw_md5, header_hash, 
                                           ssdeep_blocksize,ssdeep_hash1,ssdeep_hash2, tlsh, vhash])
            
            except KeyError:
                false_count+=1
                #non_fileType.append(new_data)
                pass

    return total,win_count

if __name__ == "__main__":
    #  Connect with the s3 bucket
    s3 = boto3.resource('s3',region_name='ap-southeast-2', aws_access_key_id='AKIA2YM7AQDFU2AWLFDK', aws_secret_access_key='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')
    bucket = s3.Bucket('malwarefeeds2.store.ronin.csiro.cloud')
    files = 1

    # Create files to save Feeds Metadata for the first time!!

    # feeds_header = ['filename','win_count','authentihash','filetype', 'codesize', 'timestamp', 'malicious', 'undetected', 'resources_len', 'sections_len','file_md5',  'sha1', 'sha256', 'imp_hash', 'icon_dhash', 'icon_raw_md5', 'header_hash', 'ssdeep_blocksize','ssdeep_hash1','ssdeep_hash2', 'tlsh', 'vhash']
    # feeds_fname = 'feedsV4.csv'
    # create_files(feeds_header,feeds_fname)

    # # # # # Create files to save resources Metadata
    # resources_header = ['filename','win_count','sha256', 'imp_hash','res_chi2','res_entropy','res_type','res_sha256','typeX']
    # resources_fname = 'resourcesV4.csv'
    # create_files(resources_header,resources_fname)

    # # # # # Create files to save section Metadata
    # sections_header = ['filename','win_count','sha256', 'imp_hash','sec_chi2','sec_entropy','sec_md5','raw_size','virtual_size','virtual_address','sec_name']
    # sections_fname = 'sectionsV4.csv'
    # create_files(sections_header,sections_fname)

    total =2042617
    win_count = 1006645
    # loop thro the obj

    # fetch from the S3 Bucket
    objectsall=list(bucket.objects.all())

    objectcurrent=objectsall[10021:]
    print(f'obj key {len(objectcurrent)}')


    # Loop through the feeds files and extract features from each feed!
    with open('scannedfiles.csv', 'a', encoding='UTF8') as scannedf:
        filenames_writer = csv.writer(scannedf)

        for obj in objectcurrent:
            
        
            filename = obj.key
            

            filenames_writer.writerow([filename])

            response = obj.get() 

            minfeed = response['Body'].read()

            minfeed_list=eval(minfeed)
            
            print(f'filename: {filename} -- files: {files}') 
            files+=1

            total,win_count=extract_features(filename,minfeed_list,total,win_count)

                #print(minfeed)
                
            if win_count ==1061151:
                 break
